{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f23b78b-0797-41b6-8096-065393e51873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [('Heavier', 'NNP'), ('bodies', 'NNS'), ('fall', 'VBP'), ('faster', 'RB'), ('.', '.')]\n",
      "B: [('All', 'DT'), ('bodies', 'NNS'), ('fall', 'VBP'), ('at', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('speed', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "A = \"Heavier bodies fall faster.\"\n",
    "B = \"All bodies fall at the same speed.\"\n",
    "\n",
    "tokens_A = word_tokenize(A)   # inglés por defecto\n",
    "tokens_B = word_tokenize(B)\n",
    "\n",
    "tags_A = pos_tag(tokens_A)    # usa eng por defecto\n",
    "tags_B = pos_tag(tokens_B)\n",
    "\n",
    "print(\"A:\", tags_A)\n",
    "print(\"B:\", tags_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2008fb82-2e46-4534-a31f-60dbf2cbd042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Heavier', 'bodies', 'fall', 'faster']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paquete Natural language toolkit en https://www.nltk.org/\n",
    "# En GitHub https://github.com/nltk/nltk\n",
    "\n",
    "import nltk\n",
    "sentence = \"\"\"Heavier bodies fall faster\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "007d3b47-bec4-4eaf-96ed-85d54d5e6121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a3ab8d7-5e98-4b52-9d0d-fee10ab0a909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Heavier bodies fall faster.']\n",
      "['Heavier', 'bodies', 'fall', 'faster', '.']\n",
      "['All bodies fall at the same speed.']\n",
      "['All', 'bodies', 'fall', 'at', 'the', 'same', 'speed', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "print(sent_tokenize(A))\n",
    "print(word_tokenize(A))\n",
    "\n",
    "print(sent_tokenize(B))\n",
    "print(word_tokenize(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb49cdc-c7de-4f7d-a07f-cee2468680d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Heavier', 'NNP'), ('bodies', 'NNS'), ('fall', 'VBP'), ('faster', 'RB'), ('.', '.')]\n",
      "[('All', 'DT'), ('bodies', 'NNS'), ('fall', 'VBP'), ('at', 'IN'), ('the', 'DT'), ('same', 'JJ'), ('speed', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "print(pos_tag(word_tokenize(A)))\n",
    "print(pos_tag(word_tokenize(B)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34b074d2-352f-4946-97e8-29ddd3d445a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heavier', 'bodies', 'fall', 'faster']\n",
      "['bodies', 'fall', 'speed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in word_tokenize(text.lower()) if w.isalpha() and w not in sw]\n",
    "\n",
    "print(remove_stopwords(A))\n",
    "print(remove_stopwords(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f7e8d12-0c4b-4fa9-9c3a-ad5218f8c5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heavier', 'bodi', 'fall', 'faster']\n",
      "['bodi', 'fall', 'speed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(text):\n",
    "    return [stemmer.stem(w) for w in remove_stopwords(text)]\n",
    "\n",
    "print(stem(A))\n",
    "print(stem(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2d92587-5992-4f6b-9ba5-b294026eab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['heavier', 'body', 'fall', 'faster']\n",
      "['body', 'fall', 'speed']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in remove_stopwords(text)]\n",
    "\n",
    "print(lemmatize(A))\n",
    "print(lemmatize(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b31d0f6-38e0-42f7-9b5c-1693a83253c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 4 samples and 4 outcomes>\n",
      "<FreqDist with 3 samples and 3 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "print(FreqDist(stem(A)))\n",
    "print(FreqDist(stem(B)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3be5c5fe-f732-4ad4-b0d9-30c3b2f256d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Heavier', 'bodies'), ('bodies', 'fall'), ('fall', 'faster'), ('faster', '.')]\n",
      "[('Heavier', 'bodies', 'fall'), ('bodies', 'fall', 'faster'), ('fall', 'faster', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "\n",
    "print(list(bigrams(word_tokenize(A))))\n",
    "print(list(trigrams(word_tokenize(A))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97396973-6ce1-4347-a037-627f5c905b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Heavier/NNP bodies/NNS) (VP fall/VBP faster/RB) ./.)\n",
      "(S\n",
      "  (NP All/DT bodies/NNS)\n",
      "  (VP fall/VBP)\n",
      "  at/IN\n",
      "  (NP the/DT same/JJ speed/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar = r\"\"\"\n",
    "NP: {<DT>?<JJ.*>*<NN.*>+}\n",
    "VP: {<VB.*><RB.*>*}\n",
    "\"\"\"\n",
    "\n",
    "from nltk import RegexpParser\n",
    "\n",
    "chunker = RegexpParser(grammar)\n",
    "\n",
    "tree_A = chunker.parse(pos_tag(word_tokenize(A)))\n",
    "tree_B = chunker.parse(pos_tag(word_tokenize(B)))\n",
    "\n",
    "print(tree_A)\n",
    "print(tree_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "644fb9e9-c4cf-4552-83d1-922a87606fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('heavy.a.01'), Synset('heavy.a.02'), Synset('heavy.a.03'), Synset('heavy.a.04'), Synset('fleshy.s.01'), Synset('clayey.s.02'), Synset('heavy.s.07'), Synset('heavy.a.08'), Synset('heavy.a.09'), Synset('heavy.s.10'), Synset('dense.s.01'), Synset('heavy.s.12'), Synset('heavy.s.13'), Synset('big.s.06'), Synset('heavy.s.15'), Synset('intemperate.s.03'), Synset('grave.s.03'), Synset('heavy.s.18'), Synset('heavy.s.19'), Synset('heavy.s.20'), Synset('heavy.s.21'), Synset('heavy.s.22'), Synset('heavy.s.23'), Synset('arduous.s.01'), Synset('heavy.s.25'), Synset('heavy.s.26'), Synset('big.s.13')]\n",
      "[Synset('speed.n.01'), Synset('speed.n.02'), Synset('speed.n.03'), Synset('focal_ratio.n.01'), Synset('amphetamine.n.01'), Synset('rush.v.01'), Synset('accelerate.v.01'), Synset('travel_rapidly.v.01'), Synset('speed.v.04'), Synset('accelerate.v.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def synsets(word):\n",
    "    return wn.synsets(word)\n",
    "\n",
    "print(synsets(\"heavier\"))\n",
    "print(synsets(\"speed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88f5c92e-3a2d-47cc-8e01-672d51474d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "def jaccard(a, b):\n",
    "    return len(set(a) & set(b)) / len(set(a) | set(b))\n",
    "\n",
    "print(jaccard(stem(A), stem(B)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0630234a-4199-4f72-9098-b823d320027b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTHER\n",
      "UNIVERSAL_EQUALITY\n"
     ]
    }
   ],
   "source": [
    "def classify(sentence):\n",
    "    tags = pos_tag(word_tokenize(sentence))\n",
    "    if any(tag in (\"JJR\", \"RBR\") for _, tag in tags):\n",
    "        return \"COMPARATIVE_CLAIM\"\n",
    "    if \"all\" in sentence.lower() and \"same\" in sentence.lower():\n",
    "        return \"UNIVERSAL_EQUALITY\"\n",
    "    return \"OTHER\"\n",
    "\n",
    "print(classify(A))\n",
    "print(classify(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ae4c2b2-ecf1-47ad-935c-112ce67ed92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Instalando: maxent_ne_chunker_tab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Biempi01\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (GPE Heavier/NNP) bodies/NNS fall/VBP faster/RB ./.)\n",
      "(S All/DT bodies/NNS fall/VBP at/IN the/DT same/JJ speed/NN ./.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "def ensure(resource_path: str, package: str):\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "        return\n",
    "    except LookupError:\n",
    "        print(f\"[INFO] Instalando: {package}\")\n",
    "        nltk.download(package)\n",
    "        nltk.data.find(resource_path)  # si falla aquí, te enteras inmediatamente\n",
    "\n",
    "# Requisitos para tu pipeline (por tu historial de errores)\n",
    "ensure(\"tokenizers/punkt\", \"punkt\")\n",
    "ensure(\"tokenizers/punkt_tab/english/\", \"punkt_tab\")\n",
    "ensure(\"taggers/averaged_perceptron_tagger_eng/\", \"averaged_perceptron_tagger_eng\")\n",
    "\n",
    "# Requisitos para NER chunking en tu NLTK\n",
    "ensure(\"chunkers/maxent_ne_chunker_tab/english_ace_multiclass/\", \"maxent_ne_chunker_tab\")\n",
    "ensure(\"corpora/words\", \"words\")\n",
    "\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "A = \"Heavier bodies fall faster.\"\n",
    "B = \"All bodies fall at the same speed.\"\n",
    "\n",
    "print(ne_chunk(pos_tag(word_tokenize(A))))\n",
    "print(ne_chunk(pos_tag(word_tokenize(B))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "affaef76-42a5-4b64-88b7-ee30c3fe5640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: {'status': 'PHYSICALLY_SUSPECT', 'reason': 'Assumes mass alone determines falling speed.', 'model': 'Contradicted by classical mechanics.'}\n",
      "B: {'status': 'PHYSICALLY_INCOMPLETE', 'reason': 'True only under restricted conditions (vacuum).', 'model': 'Condition-dependent truth.'}\n"
     ]
    }
   ],
   "source": [
    "# Ejemplos de introducción de heurísticas\n",
    "\n",
    "def physical_heuristic(sentence: str):\n",
    "    s = sentence.lower()\n",
    "\n",
    "    if \"heavier\" in s and \"faster\" in s:\n",
    "        return {\n",
    "            \"status\": \"PHYSICALLY_SUSPECT\",\n",
    "            \"reason\": \"Assumes mass alone determines falling speed.\",\n",
    "            \"model\": \"Contradicted by classical mechanics.\"\n",
    "        }\n",
    "\n",
    "    if \"all bodies\" in s and \"same speed\" in s:\n",
    "        return {\n",
    "            \"status\": \"PHYSICALLY_INCOMPLETE\",\n",
    "            \"reason\": \"True only under restricted conditions (vacuum).\",\n",
    "            \"model\": \"Condition-dependent truth.\"\n",
    "        }\n",
    "\n",
    "    return {\n",
    "        \"status\": \"UNKNOWN\",\n",
    "        \"reason\": \"No heuristic rule matched.\"\n",
    "    }\n",
    "\n",
    "A = \"Heavier bodies fall faster.\"\n",
    "B = \"All bodies fall at the same speed.\"\n",
    "\n",
    "print(\"A:\", physical_heuristic(A))\n",
    "print(\"B:\", physical_heuristic(B))\n",
    "\n",
    "# La heurística no decide la verdad, decide la consistencia con un modelo y grado de especificación\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "575e8104-475b-4b90-a8f9-a6cfaa917660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exaple introcuction heuristics with a decission tre (no semantics, LLMs included)\n",
    "\n",
    "import re\n",
    "\n",
    "A = \"Heavier bodies fall faster.\"\n",
    "B = \"All bodies fall at the same speed.\"\n",
    "\n",
    "def detect_claim_type(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    if re.search(r\"\\bheavier\\b\", s) and re.search(r\"\\bfaster\\b\", s) and re.search(r\"\\bfall\\b\", s):\n",
    "        return \"MASS_IMPLIES_FASTER\"\n",
    "    if re.search(r\"\\ball\\b\", s) and re.search(r\"\\bsame speed\\b\", s) and re.search(r\"\\bfall\\b\", s):\n",
    "        return \"UNIVERSAL_SAME_SPEED\"\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def needs_context(claim_type: str) -> bool:\n",
    "    # Ambas suelen necesitar condiciones para evaluarlas \"respecto a la realidad\"\n",
    "    return claim_type in {\"MASS_IMPLIES_FASTER\", \"UNIVERSAL_SAME_SPEED\"}\n",
    "\n",
    "def ask_questions(claim_type: str):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de preguntas mínimas para desambiguar.\n",
    "    \"\"\"\n",
    "    qs = []\n",
    "    # Pregunta 1: medio (vacío vs aire) es la más determinante\n",
    "    qs.append(\"¿Estamos en vacío (vacuum) o en aire (air)? Responde: vacuum/air\")\n",
    "    # Pregunta 2: si hay aire, la forma/área importa por arrastre\n",
    "    qs.append(\"¿Los cuerpos tienen la misma forma y área frontal? Responde: same_shape/different_shape\")\n",
    "    # Pregunta 3: si quieres, precisión adicional\n",
    "    qs.append(\"¿Comparas 'velocidad' instantánea o 'aceleración' (acceleration)? Responde: speed/acceleration\")\n",
    "    return qs\n",
    "\n",
    "def evaluate_with_context(sentence: str, ctx: dict):\n",
    "    \"\"\"\n",
    "    Dictamen condicional basado en un modelo físico simplificado:\n",
    "    - En vacío: misma aceleración g (ignorando relatividad).\n",
    "    - En aire: depende fuertemente del arrastre (forma/área), no solo masa.\n",
    "    \"\"\"\n",
    "    claim_type = detect_claim_type(sentence)\n",
    "\n",
    "    medium = ctx.get(\"medium\")              # 'vacuum'/'air'\n",
    "    shape = ctx.get(\"shape\")                # 'same_shape'/'different_shape'\n",
    "    measure = ctx.get(\"measure\")            # 'speed'/'acceleration'\n",
    "\n",
    "    # Si falta algo, no inventamos.\n",
    "    missing = [k for k in (\"medium\", \"shape\", \"measure\") if ctx.get(k) is None]\n",
    "    if missing:\n",
    "        return {\n",
    "            \"status\": \"NEED_MORE_CONTEXT\",\n",
    "            \"missing\": missing,\n",
    "            \"note\": \"No se puede evaluar sin fijar condiciones.\"\n",
    "        }\n",
    "\n",
    "    # Evaluación\n",
    "    if claim_type == \"MASS_IMPLIES_FASTER\":\n",
    "        if medium == \"vacuum\":\n",
    "            return {\n",
    "                \"status\": \"FALSE_UNDER_CTX\",\n",
    "                \"because\": \"En vacío, la aceleración no depende de la masa.\",\n",
    "                \"detail\": \"Bajo el modelo clásico (Galileo/Newton), masa ≠ caída más rápida en vacío.\"\n",
    "            }\n",
    "        if medium == \"air\":\n",
    "            # En aire, puede ocurrir que un objeto más pesado caiga más rápido,\n",
    "            # pero NO por ser más pesado en sí, sino por la relación peso/arrastre y la forma.\n",
    "            if shape == \"same_shape\":\n",
    "                return {\n",
    "                    \"status\": \"CONDITIONALLY_PLAUSIBLE_BUT_MISATTRIBUTED\",\n",
    "                    \"because\": \"En aire, con misma forma, el más pesado puede alcanzar mayor velocidad terminal.\",\n",
    "                    \"detail\": \"Pero la causa relevante es el arrastre (drag) y la relación peso/área, no 'masa por sí sola'.\"\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"UNDETERMINED_UNDER_CTX\",\n",
    "                    \"because\": \"Con formas distintas, el arrastre domina; la masa por sí sola no predice el resultado.\",\n",
    "                    \"detail\": \"Necesitarías parámetros de drag/área/densidad para decidir.\"\n",
    "                }\n",
    "\n",
    "    if claim_type == \"UNIVERSAL_SAME_SPEED\":\n",
    "        if medium == \"vacuum\":\n",
    "            # “same speed” es menos correcto que “same acceleration”\n",
    "            if measure == \"acceleration\":\n",
    "                return {\n",
    "                    \"status\": \"TRUE_UNDER_CTX\",\n",
    "                    \"because\": \"En vacío, todos comparten la misma aceleración g.\",\n",
    "                    \"detail\": \"Es la formulación correcta del principio galileano.\"\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"MISSTATED_BUT_RELATED\",\n",
    "                    \"because\": \"En vacío, lo igual es la aceleración, no la velocidad en todo instante.\",\n",
    "                    \"detail\": \"La velocidad depende del tiempo y condiciones iniciales; con mismas condiciones iniciales, sí coinciden.\"\n",
    "                }\n",
    "        if medium == \"air\":\n",
    "            return {\n",
    "                \"status\": \"FALSE_UNDER_CTX\",\n",
    "                \"because\": \"En aire, distintas masas/formas tienen distinta velocidad terminal y dinámica por arrastre.\",\n",
    "                \"detail\": \"La igualdad universal de velocidad no se sostiene sin restricciones fuertes.\"\n",
    "            }\n",
    "\n",
    "    return {\n",
    "        \"status\": \"UNKNOWN\",\n",
    "        \"because\": \"La frase no coincide con patrones del agente.\",\n",
    "        \"claim_type\": claim_type\n",
    "    }\n",
    "\n",
    "def run_agent(sentence: str):\n",
    "    claim_type = detect_claim_type(sentence)\n",
    "    print(f\"Frase: {sentence}\")\n",
    "    print(f\"Tipo detectado: {claim_type}\")\n",
    "\n",
    "    if not needs_context(claim_type):\n",
    "        print(\"No tengo reglas para esta frase.\")\n",
    "        return\n",
    "\n",
    "    # Preguntas mínimas\n",
    "    ctx = {\"medium\": None, \"shape\": None, \"measure\": None}\n",
    "    for q in ask_questions(claim_type):\n",
    "        ans = input(q + \"\\n> \").strip().lower()\n",
    "        if ans in (\"vacuum\", \"air\"):\n",
    "            ctx[\"medium\"] = ans\n",
    "        elif ans in (\"same_shape\", \"different_shape\"):\n",
    "            ctx[\"shape\"] = ans\n",
    "        elif ans in (\"speed\", \"acceleration\"):\n",
    "            ctx[\"measure\"] = ans\n",
    "        else:\n",
    "            print(\"Respuesta no reconocida. Dejo ese campo como desconocido.\")\n",
    "\n",
    "    result = evaluate_with_context(sentence, ctx)\n",
    "    print(\"\\nDICTAMEN:\")\n",
    "    for k, v in result.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "# Ejemplos de ejecución:\n",
    "# run_agent(A)\n",
    "# run_agent(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe455468-f3b7-4262-910f-fbc8db64c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: Heavier bodies fall faster.\n",
      "Tipo detectado: MASS_IMPLIES_FASTER\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "¿Estamos en vacío (vacuum) o en aire (air)? Responde: vacuum/air\n",
      ">  air\n",
      "¿Los cuerpos tienen la misma forma y área frontal? Responde: same_shape/different_shape\n",
      ">  different_shape\n",
      "¿Comparas 'velocidad' instantánea o 'aceleración' (acceleration)? Responde: speed/acceleration\n",
      ">  speed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DICTAMEN:\n",
      "- status: UNDETERMINED_UNDER_CTX\n",
      "- because: Con formas distintas, el arrastre domina; la masa por sí sola no predice el resultado.\n",
      "- detail: Necesitarías parámetros de drag/área/densidad para decidir.\n"
     ]
    }
   ],
   "source": [
    "run_agent(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d6b9aa0e-79bd-4c07-90a3-83ed48463d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\biempi01\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa1cc82-dd34-4eb1-9379-038530eaaff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': 'Heavier bodies fall faster.', 'top_matches': [('ARISTOTLE_STYLE', 'Heavier objects fall faster (an Aristotelian claim).', 0.7986726760864258), ('VACUUM_EQUAL_ACCEL', 'In a vacuum, all objects fall with the same acceleration (ignoring relativity).', 0.5380131006240845), ('AIR_DRAG_MATTERS', 'In air, drag depends on shape and area; falling speed is not determined by mass alone.', 0.5199270248413086)], 'questions': ['Context needed: vacuum or air?']}\n",
      "{'sentence': 'All bodies fall at the same speed.', 'top_matches': [('VACUUM_EQUAL_ACCEL', 'In a vacuum, all objects fall with the same acceleration (ignoring relativity).', 0.7063049077987671), ('ARISTOTLE_STYLE', 'Heavier objects fall faster (an Aristotelian claim).', 0.6049437522888184), ('AIR_DRAG_MATTERS', 'In air, drag depends on shape and area; falling speed is not determined by mass alone.', 0.5472099184989929)], 'questions': ['Context needed: vacuum or air?']}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "A = \"Heavier bodies fall faster.\"\n",
    "B = \"All bodies fall at the same speed.\"\n",
    "\n",
    "# Mini “base de conocimiento” (plantillas)\n",
    "KB = [\n",
    "    (\"VACUUM_EQUAL_ACCEL\", \"In a vacuum, all objects fall with the same acceleration (ignoring relativity).\"),\n",
    "    (\"AIR_DRAG_MATTERS\", \"In air, drag depends on shape and area; falling speed is not determined by mass alone.\"),\n",
    "    (\"SPEED_VS_ACCEL\", \"Equal acceleration is not the same as equal speed at all times unless initial conditions match.\"),\n",
    "    (\"ARISTOTLE_STYLE\", \"Heavier objects fall faster (an Aristotelian claim).\"),\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "kb_texts = [t for _, t in KB]\n",
    "kb_emb = model.encode(kb_texts, convert_to_tensor=True)\n",
    "\n",
    "def analyze(sentence: str, topk=3):\n",
    "    emb = model.encode(sentence, convert_to_tensor=True)\n",
    "    sims = util.cos_sim(emb, kb_emb)[0]\n",
    "    ranked = sorted(list(enumerate(sims)), key=lambda x: float(x[1]), reverse=True)[:topk]\n",
    "    hits = [(KB[i][0], KB[i][1], float(score)) for i, score in ranked]\n",
    "\n",
    "    # Heurística mínima basada en hits (pero guiada por embeddings)\n",
    "    labels = [h[0] for h in hits]\n",
    "    questions = []\n",
    "    if \"VACUUM_EQUAL_ACCEL\" in labels or \"AIR_DRAG_MATTERS\" in labels:\n",
    "        questions.append(\"Context needed: vacuum or air?\")\n",
    "    if \"SPEED_VS_ACCEL\" in labels:\n",
    "        questions.append(\"Clarify: are you claiming same speed or same acceleration?\")\n",
    "\n",
    "    return {\n",
    "        \"sentence\": sentence,\n",
    "        \"top_matches\": hits,\n",
    "        \"questions\": questions\n",
    "    }\n",
    "\n",
    "print(analyze(A))\n",
    "print(analyze(B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32db2f-4d31-4f38-9298-5de39f489d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
